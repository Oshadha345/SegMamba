{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü©ª BratsMamba: 3D Brain Tumor Segmentation with State Space Model\n",
    "\n",
    "---\n",
    "\n",
    "Content : \n",
    "\n",
    "- Problem Statement\n",
    "- Clinical Context\n",
    "- The Challenge\n",
    "- Solution Overview\n",
    "- Key Technical Features\n",
    "- Referred Sources(Research Papers, Codebases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîµ **Problem Statement**\n",
    "\n",
    "Our main goal is to capture both fine-grained tumor boundaries (Necrosis) and global contextual features (Edema) efficiently in MRI tumor images. This is a heavily researched and critical area vital for bio-medical engineering field. This problem is in the domain of computer vision, specifically semantic segmentation tasks. \n",
    "\n",
    "Semantic segmentation is a critical task in computer vision, requiring the precise classification of every pixel in an image. Traditional Convolutional Neural Networks (CNNs) like U-Net excel at capturing local features but often struggle with long-range dependencies due to their limited receptive fields. Transformers addressed this with self-attention, but at the cost of quadratic computational complexity($O(N^2)$), making them heavy for high-resolution tasks.\n",
    "\n",
    "### üü£ **Clinical Context**\n",
    "\n",
    "Gliomas are the most common primary brain malignancies. Treatment planning (surgery and radiotherapy) relies heavily on the precise delineation of tumor sub-regions:\n",
    "1.  **Necrotic Core (NCR):** Dead tissue, indicates tumor aggressiveness.\n",
    "2.  **Peritumoral Edema (ED):** Swelling around the tumor, critical for radiotherapy margins.\n",
    "3.  **Enhancing Tumor (ET):** Active tumor cells, the primary target for surgical resection.\n",
    "\n",
    "### üü° **The Challenge**\n",
    "\n",
    "Manual segmentation by radiologists is tedious, time-consuming (hours per patient), and prone to inter-observer variability. An automated, accurate 3D segmentation tool can:\n",
    "*   **Accelerate Diagnosis:** Reduce analysis time from hours to seconds.\n",
    "*   **Standardize Care:** Provide consistent, objective measurements for longitudinal tracking.\n",
    "*   **Surgical Planning:** Enable precise 3D mapping for neuro-navigation systems to preserve healthy brain tissue.\n",
    "\n",
    "\n",
    "### üü¢ **Solution Overview**\n",
    "\n",
    "We use a hybrid architecture that leverages the strengths of both `CNNs` and `State Space Models (SSMs)` to efficiently capture both local and global features in MRI images.\n",
    "\n",
    "**Why Mamba?** We address this trade-off using **Mamba-SSM (State Space Models)**. Mamba offers linear complexity ($O(N)$) with respect to sequence length, allowing us to model global context (long-range dependencies) without the massive memory overhead of Transformers.\n",
    "\n",
    "**The Solution: BratsMamba** \n",
    "This notebook implements BratsMamba, a hybrid architecture that combines the hierarchical structure of a U-Net with Mamba blocks. This allows us to capture:\n",
    "1. **Local Texture Details**: Via convolutional stems and decoder blocks.\n",
    "3. **Global Semantic Context**: Via Mamba encoders that scan the image as a sequence, understanding the \"whole picture\" efficiently.\n",
    "\n",
    "### üî¥ **Key Technical Features**\n",
    "\n",
    "* **Architecture:** Dual-Path Conv Stem + Mamba Encoder/Decoder + U-Net Skip Connections.\n",
    "* **Data Pipeline:** Lazy loading from internal disk (`/tmp`) to handle large datasets without RAM explosion.\n",
    "* **Robustness:** Implements `SpatialPadd` and `DivisiblePadd` to handle variable MRI volume sizes preventing shape mismatches.\n",
    "* **Evaluation:** Clinical metrics (Dice & HD95) calculated on Whole Tumor (WT), Tumor Core (TC), and Enhancing Tumor (ET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è¨ Imports\n",
    "\n",
    "---\n",
    "\n",
    "Importing and installing necessary libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library choices and their source links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize a stack of open-source, high-performance libraries specifically chosen for 3D medical imaging and efficient sequence modeling.\n",
    "\n",
    "| Library | Badge | Reason for Choice | Source / Documentation |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **PyTorch** | ![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white) | The core deep learning framework used for tensor operations, automatic differentiation, and model building. Chosen for its dynamic computation graph and extensive ecosystem. | [pytorch.org](https://pytorch.org/) |\n",
    "| **MONAI** | ![MONAI](https://img.shields.io/badge/MONAI-%237B61FF.svg?style=flat&logo=MONAI&logoColor=white) | Specialized framework for medical AI. We use it for 3D specific transforms (`RandCropByPosNegLabeld`), losses (`DiceCELoss`), and metrics (`Hausdorff Distance`). | [monai.io](https://monai.io/) |\n",
    "| **Mamba-SSM** | ![Mamba](https://img.shields.io/badge/Mamba--SSM-black?style=flat) | Provides the state-space model implementation. Chosen for its linear complexity $O(N)$ allowing efficient processing of long sequences from flattened 3D volumes. | [GitHub](https://github.com/state-spaces/mamba) |\n",
    "| **Nibabel** | ![Nibabel](https://img.shields.io/badge/Nibabel-blue?style=flat) | Essential for reading and writing NIfTI (`.nii.gz`) neuroimaging file formats used in the BraTS dataset. | [nipy.org/nibabel](https://nipy.org/nibabel/) |\n",
    "| **Einops** | ![Einops](https://img.shields.io/badge/Einops-green?style=flat) | Used for readable and reliable tensor reshaping and rearranging, critical for the Mamba block implementation. | [GitHub](https://github.com/arogozhnikov/einops) |\n",
    "\n",
    "### ‚úÖ Compliance with Competition Criteria\n",
    "\n",
    "All functions and libraries used in this notebook strictly adhere to the allowed resources:\n",
    "\n",
    "*   **‚úî Open Source Packages:** We strictly utilize publicly available Python packages (PyTorch, MONAI, NumPy, Pandas, etc.).\n",
    "*   **‚úî Open Source Dataset:** The BraTS 2021 dataset is a publicly available benchmark from the RSNA-ASNR-MICCAI challenge.\n",
    "*   **‚úî Documented Architecture:** The **BraTSMamba** architecture is a custom implementation documented within this notebook, combining concepts from U-Net and Mamba-SSM papers.\n",
    "*   **‚ùå No AutoML / Pre-built Pipelines:** The training loop, data loading, and model architecture are implemented from scratch (or using modular building blocks) rather than using \"black-box\" AutoML tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T13:27:33.344220Z",
     "iopub.status.busy": "2025-12-31T13:27:33.343786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Checking and Installing Dependencies...\n",
      "   ‚úÖ Mamba-SSM already installed.\n",
      "‚úÖ Dependencies ready in 5.4s\n",
      "üìÇ Importing Libraries...\n",
      "‚úÖ Configuration Complete. Artifacts will be saved to: /storage2/ChangeDetection/NSST-mamba/mamba_decoder/UrbanMamba/outputs/bratsmamba\n",
      "\n",
      "üöÄ System Ready.\n",
      "   PyTorch: 2.5.1\n",
      "   Device:  cuda:0\n",
      "   GPU:     Quadro GV100\n",
      "   Memory:  34.08 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# general imports\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import tarfile\n",
    "import subprocess\n",
    "import warnings\n",
    "import random\n",
    "import shutil\n",
    "import glob as gb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# suppress cluttered warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# mamba-ssm, monai, nibabel and einops\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"‚öôÔ∏è Checking and Installing Dependencies...\")\n",
    "start_install = time.time()\n",
    "\n",
    "# Helper to install if missing\n",
    "def install_package(package_name, pip_name=None):\n",
    "    if pip_name is None: pip_name = package_name\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "    except ImportError:\n",
    "        print(f\"   Installing {pip_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name, \"--quiet\"])\n",
    "\n",
    "# 1. Medical / 3D Imaging Libraries\n",
    "install_package(\"nibabel\")\n",
    "install_package(\"monai\")\n",
    "install_package(\"einops\") # needed for tensor rearranging in Mamba/Transformers\n",
    "\n",
    "# 2. Mamba-SSM & Causal Conv1d\n",
    "# We prioritize pre-built wheels to save time.\n",
    "try:\n",
    "    import mamba_ssm\n",
    "    print(\"   ‚úÖ Mamba-SSM already installed.\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è Mamba-SSM not found. Installing specific versions for Kaggle T4...\")\n",
    "    try:\n",
    "        # Install causal-conv1d first\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"causal-conv1d>=1.2.0\"])\n",
    "        # Install mamba-ssm\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mamba-ssm\"])\n",
    "        print(\"   ‚úÖ Mamba-SSM installed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error installing Mamba: {e}\")\n",
    "        print(\"   -> Ensure you are using GPU T4 and Internet is ON.\")\n",
    "\n",
    "print(f\"‚úÖ Dependencies ready in {time.time() - start_install:.1f}s\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1.2 other libraries\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"üìÇ Importing Libraries...\")\n",
    "\n",
    "# > standard data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# > PyTorch & DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler  # mixed Precision\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# > Medical / Specialized Imaging (MONAI & Nibabel)\n",
    "import nibabel as nib\n",
    "\n",
    "# MONAI & Medical Imaging Imports\n",
    "from monai.utils import set_determinism\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.networks.nets import UNet, UNETR, SwinUNETR # to benchmark\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import DataLoader, Dataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped, \n",
    "    ScaleIntensityd, RandCropByPosNegLabeld, RandFlipd, \n",
    "    RandShiftIntensityd, SpatialPadd, DivisiblePadd, AsDiscrete, MapLabelValued\n",
    ")\n",
    "\n",
    "# > Math & Tensor Manipulation\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# > Scikit-Learn (Metrics & Splitting)\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score\n",
    "\n",
    "# Set Reproducibility\n",
    "SEED = 42\n",
    "set_determinism(seed=SEED)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Configuration Complete.\")\n",
    "print(f\"\\nüöÄ System Ready.\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Device:  {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU:     {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory:  {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ DataSet - BraTS 2021(Brain Tumor Segmentation)\n",
    "\n",
    "---\n",
    "\n",
    "Content :\n",
    "\n",
    "- Why `BraTS 2021 Task 01` Dataset ?\n",
    "- Dataset Information\n",
    "- Datasets path setup\n",
    "- Dataset Unpacking & Discovery\n",
    "- Dataset analysis\n",
    "  - Visualizing BraTS Dataset Samples\n",
    "  - Statistics(Class Statisitcs,...)\n",
    "  - Visualization\n",
    "  - What preprocessings needed before using it to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why `BraTS 2021 Task 01` Dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the **BraTS Challenge 2021 Dataset**, the global benchmark for medical 3D segmentation. This dataset is uniquely suited for **BratMamba** because:\n",
    "\n",
    "1. **3D Volumetric Data**: Unlike 2D datasets, BraTS provides full 3D MRI volumes ($240 \\times 240 \\times 155$). This justifies the use of Mamba-SSM, which excels at modeling the extremely long sequences created by flattening 3D volumes ($N = H \\times W \\times D$), a task where Transformers typically run out of memory.\n",
    "   \n",
    "2. **Multi-Modal Complexity**: Each patient has 4 modalities (T1, T1c, T2, FLAIR). Our \"Dual CNN Stem\" is designed specifically to fuse these heterogeneous signals locally before global processing.\n",
    "   \n",
    "3. **Class Imbalance**: The tumor sub-regions (Necrosis, Edema, Enhancing) vary wildly in size, requiring robust loss functions (Dice/Focal) rather than simple accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Segmentation Classes & Labels*\n",
    "\n",
    "We follow the standard BraTS protocol:\n",
    "* Label 0: Background\n",
    "* Label 1 (NCR): Necrotic Tumor Core (Hypointense on T1-Gd)\n",
    "* Label 2 (ED): Peritumoral Edema (Hyperintense on FLAIR)\n",
    "* Label 4 (ET): Enhancing Tumor (Hyperintense on T1-Gd)\n",
    "\n",
    "\n",
    "*Dataset Paper Citation*:\n",
    "\n",
    "[1]U. Baid et al., ‚ÄúThe RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification,‚Äù arXiv:2107.02314 [cs], Sep. 2021, Available: https://arxiv.org/abs/2107.02314\n",
    "\n",
    "*Evaluation Metrics*\n",
    "\n",
    "To ensure fair comparison with SOTA, we track:\n",
    "1. **Dice Similarity Coefficient (DSC)**: Measures overlap accuracy.\n",
    "2. **Hausdorff Distance (HD95)**: Measures the worst-case boundary error (critical for surgical planning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path for dataset\n",
    "BASE_PATH = \"/storage2/ChangeDetection/Datasets/Loveda/NSST_27ch\"\n",
    "\n",
    "# Dataset tar file path \n",
    "TAR_PATH = \"/storage2/ChangeDetection/Datasets/Loveda/NSST_27ch/BraTS2021_Training_Data.tar\"\n",
    "\n",
    "# create extract path if doesn't exist\n",
    "EXTRACT_PATH = os.path.join(BASE_PATH, \"BraTs\") \n",
    "if not os.path.exists(EXTRACT_PATH):\n",
    "    os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
    "\n",
    "# create  directory to store normalized dataset\n",
    "NORMALIZED_DATA_PATH = os.path.join(BASE_PATH, \"processed_data\")\n",
    "if not os.path.exists(NORMALIZED_DATA_PATH):\n",
    "    os.makedirs(NORMALIZED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Create Artifacts Directory\n",
    "ARTIFACTS_DIR = \"/storage2/ChangeDetection/NSST-mamba/mamba_decoder/UrbanMamba/BraTSMamba/outputs/\"\n",
    "if not os.path.exists(ARTIFACTS_DIR):\n",
    "    os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "    \n",
    "# Folder to store raw dataset results\n",
    "RAWDATA_PATH = os.path.join(ARTIFACTS_DIR, \"raw_dataset_results\")\n",
    "if not os.path.exists(RAWDATA_PATH):\n",
    "    os.makedirs(RAWDATA_PATH, exist_ok=True)\n",
    "    \n",
    "# Folder to store processed dataset results\n",
    "PROCDATARESULTS_PATH = os.path.join(ARTIFACTS_DIR, \"processed_dataset_results\")\n",
    "if not os.path.exists(PROCDATARESULTS_PATH):\n",
    "    os.makedirs(PROCDATARESULTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Unpacking & Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def unpack_dataset(tar_path, extract_path, limit=None):\n",
    "    \"\"\"Extracts dataset from TAR to local storage if not already unpacked.\"\"\"\n",
    "    print(f\"üì¶ Source: {tar_path}\")\n",
    "\n",
    "    existing = gb.glob(os.path.join(extract_path, \"BraTS2021_*\", \"*_flair.nii.gz\"), recursive=True)\n",
    "    if existing:\n",
    "        print(f\"‚úÖ Data already unpacked ({len(existing)} cases). Skipping...\")\n",
    "        return\n",
    "\n",
    "    print(\"‚è≥ Unpacking... (This utilizes internal disk IO)\")\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = tar.getmembers()\n",
    "        count = 0\n",
    "        for member in tqdm(members, desc=\"Extracting\"):\n",
    "            tar.extract(member, path=extract_path)\n",
    "            count += 1\n",
    "    print(f\"‚úÖ Extracted {count} files.\")\n",
    "    \n",
    "def get_file_lists(data_dir):\n",
    "    \"\"\"Builds BraTS case list from NIfTI files.\"\"\"\n",
    "    print(f\"üîç Scanning: {data_dir}\")\n",
    "    flair_files = sorted(gb.glob(os.path.join(data_dir, \"BraTS2021_*\", \"*_flair.nii.gz\")))\n",
    "\n",
    "    if not flair_files:\n",
    "        raise ValueError(\"‚ùå No BraTS NIfTI files found! Check dataset path.\")\n",
    "\n",
    "    data_dicts = []\n",
    "    for flair in flair_files:\n",
    "        base = flair[: -len(\"_flair.nii.gz\")]\n",
    "        t1 = base + \"_t1.nii.gz\"\n",
    "        t1ce = base + \"_t1ce.nii.gz\"\n",
    "        t2 = base + \"_t2.nii.gz\"\n",
    "        seg = base + \"_seg.nii.gz\"\n",
    "        if all(os.path.exists(p) for p in (t1, t1ce, t2, seg)):\n",
    "            data_dicts.append({\"image\": [flair, t1, t1ce, t2], \"label\": seg})\n",
    "\n",
    "    if not data_dicts:\n",
    "        raise ValueError(\"‚ùå No complete BraTS cases found.\")\n",
    "\n",
    "    return data_dicts\n",
    "\n",
    "print(\"üì¶ Extracting Source Data...\")\n",
    "\n",
    "# Execute Unpack\n",
    "unpack_dataset(TAR_PATH, EXTRACT_PATH, limit=None)\n",
    "all_files = get_file_lists(EXTRACT_PATH)\n",
    "\n",
    "\n",
    "# Dataset Count\n",
    "print(f\"‚úÖ Found {len(all_files)} complete BraTS cases.\")\n",
    "\n",
    "# example entries from dataset dict\n",
    "print(\"First two entries:\", all_files[:2])\n",
    "\n",
    "print(\"\\n‚úÖ Data Extraction and Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing BraTS Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION OF ALL 5 MODALITIES + SEGMENTATION + OVERLAY\n",
    "# =============================================================================\n",
    "def visualize_sample():\n",
    "    # Find patient directory\n",
    "    patient_dirs = sorted(gb.glob(os.path.join(EXTRACT_PATH, \"*\")))\n",
    "    patient_dirs = [d for d in patient_dirs if os.path.isdir(d)]\n",
    "    \n",
    "    if not patient_dirs:\n",
    "        print(\"‚ùå No patient directories found!\")\n",
    "        return\n",
    "    \n",
    "    # Pick 3 samples\n",
    "    sample_ids = [\"BraTS2021_01498\", \"BraTS2021_00107\", \"BraTS2021_01219\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 6, figsize=(30, 18))\n",
    "    \n",
    "    for row, sample_id in enumerate(sample_ids):\n",
    "        sample_dir = [d for d in patient_dirs if sample_id in d]\n",
    "        if not sample_dir:\n",
    "            print(f\"‚ùå {sample_id} not found!\")\n",
    "            continue\n",
    "        sample_dir = sample_dir[0]\n",
    "        p_id = os.path.basename(sample_dir)\n",
    "        \n",
    "        print(f\"üëÄ Visualizing Patient: {p_id}\")\n",
    "        \n",
    "        # Load all 5 volumes\n",
    "        flair = nib.load(os.path.join(sample_dir, f\"{p_id}_flair.nii.gz\")).get_fdata()\n",
    "        t1 = nib.load(os.path.join(sample_dir, f\"{p_id}_t1.nii.gz\")).get_fdata()\n",
    "        t1ce = nib.load(os.path.join(sample_dir, f\"{p_id}_t1ce.nii.gz\")).get_fdata()\n",
    "        t2 = nib.load(os.path.join(sample_dir, f\"{p_id}_t2.nii.gz\")).get_fdata()\n",
    "        seg = nib.load(os.path.join(sample_dir, f\"{p_id}_seg.nii.gz\")).get_fdata()\n",
    "        \n",
    "        # Find slice with max tumor area\n",
    "        slice_idx = np.argmax(np.sum(seg > 0, axis=(0, 1)))\n",
    "        \n",
    "        # 1. FLAIR\n",
    "        axes[row, 0].imshow(flair[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        axes[row, 0].set_title(\"FLAIR\", fontsize=14)\n",
    "        axes[row, 0].set_ylabel(p_id, fontsize=12, fontweight='bold')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # 2. T1\n",
    "        axes[row, 1].imshow(t1[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        axes[row, 1].set_title(\"T1\", fontsize=14)\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        # 3. T1CE\n",
    "        axes[row, 2].imshow(t1ce[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        axes[row, 2].set_title(\"T1CE\", fontsize=14)\n",
    "        axes[row, 2].axis('off')\n",
    "        \n",
    "        # 4. T2\n",
    "        axes[row, 3].imshow(t2[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        axes[row, 3].set_title(\"T2\", fontsize=14)\n",
    "        axes[row, 3].axis('off')\n",
    "        \n",
    "        # 5. Segmentation Mask (Color-coded)\n",
    "        mask_rgb = np.zeros((*seg[:, :, slice_idx].T.shape, 3))\n",
    "        mask_slice = seg[:, :, slice_idx].T\n",
    "        mask_rgb[mask_slice == 1] = [1, 0, 0]  # NCR - Red\n",
    "        mask_rgb[mask_slice == 2] = [0, 1, 0]  # ED - Green\n",
    "        mask_rgb[mask_slice == 4] = [0, 0, 1]  # ET - Blue\n",
    "        \n",
    "        axes[row, 4].imshow(mask_rgb, origin='lower')\n",
    "        axes[row, 4].set_title(\"Segmentation\\n(Red=NCR, Green=ED, Blue=ET)\", fontsize=14)\n",
    "        axes[row, 4].axis('off')\n",
    "        \n",
    "        # 6. Overlay (FLAIR + Segmentation)\n",
    "        axes[row, 5].imshow(flair[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        mask_rgba = np.zeros((*seg[:, :, slice_idx].T.shape, 4))\n",
    "        mask_rgba[mask_slice == 1] = [1, 0, 0, 0.5]  # NCR - Red\n",
    "        mask_rgba[mask_slice == 2] = [0, 1, 0, 0.5]  # ED - Green\n",
    "        mask_rgba[mask_slice == 4] = [0, 0, 1, 0.5]  # ET - Blue\n",
    "        axes[row, 5].imshow(mask_rgba, origin='lower')\n",
    "        axes[row, 5].set_title(\"FLAIR + Overlay\", fontsize=14)\n",
    "        axes[row, 5].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"BraTS 2021 Dataset Samples - All Modalities\", fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"BraTS_all_modalities.png\"), bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we analyze the dataset to understand class distributions, volume sizes, and intensity statistics. This helps inform preprocessing and augmentation strategies. And after that use these statistics first to visualize to decide what preprocessinngs needed, is there any class imbalance and what augmentations are needed before using it to train?, i.e., normalization, cropping, resizing, etc.\n",
    "\n",
    "#### List of statistics to be calculated:\n",
    "- Number of samples\n",
    "- Volume size statistics (min, max, mean, std)\n",
    "- Class distribution (number of voxels per class)\n",
    "\n",
    "#### Visualization\n",
    "- Volume Dimensions Distribution\n",
    "- Global Class Distribution (Pie Chart)\n",
    "- Class Imbalance Bar Chart\n",
    "- Per-Sample Tumor Volume Distribution\n",
    "- Per-Class Volume Distribution (Box Plot)\n",
    "- Tumor-to-Brain Ratio Distribution\n",
    "- Intensity Statistics per Modality\n",
    "- Intensity Range (Min-Max) per Modality\n",
    "- Intensity Standard Deviation Comparison\n",
    "- Correlation Between Tumor Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä COMPREHENSIVE DATASET STATISTICS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_dataset_statistics():\n",
    "    \"\"\"Calculate comprehensive statistics for BraTS 2021 dataset.\"\"\"\n",
    "    \n",
    "    print(\"üìä Calculating Dataset Statistics...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get all patient directories\n",
    "    patient_dirs = sorted(gb.glob(os.path.join(EXTRACT_PATH, \"BraTS2021_*\")))\n",
    "    patient_dirs = [d for d in patient_dirs if os.path.isdir(d)]\n",
    "    \n",
    "    if not patient_dirs:\n",
    "        print(\"‚ùå No patient directories found!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize storage for statistics\n",
    "    stats = {\n",
    "        'n_samples': 0,\n",
    "        'volumes': {'H': [], 'W': [], 'D': []},\n",
    "        'class_voxels': {0: 0, 1: 0, 2: 0, 4: 0},  # BraTS uses 0,1,2,4\n",
    "        'intensity': {\n",
    "            'flair': {'min': [], 'max': [], 'mean': [], 'std': []},\n",
    "            't1': {'min': [], 'max': [], 'mean': [], 'std': []},\n",
    "            't1ce': {'min': [], 'max': [], 'mean': [], 'std': []},\n",
    "            't2': {'min': [], 'max': [], 'mean': [], 'std': []}\n",
    "        },\n",
    "        'tumor_volumes': [],  # Total tumor volume per patient\n",
    "        'class_volumes_per_sample': {1: [], 2: [], 4: []},  # Per-sample class volumes\n",
    "        'brain_volumes': [],  # Non-zero brain region\n",
    "        'tumor_to_brain_ratio': []\n",
    "    }\n",
    "    \n",
    "    print(f\"üîç Processing {len(patient_dirs)} patients...\")\n",
    "    \n",
    "    for p_dir in tqdm(patient_dirs, desc=\"Analyzing\"):\n",
    "        p_id = os.path.basename(p_dir)\n",
    "        \n",
    "        try:\n",
    "            # Load all modalities\n",
    "            flair = nib.load(os.path.join(p_dir, f\"{p_id}_flair.nii.gz\")).get_fdata()\n",
    "            t1 = nib.load(os.path.join(p_dir, f\"{p_id}_t1.nii.gz\")).get_fdata()\n",
    "            t1ce = nib.load(os.path.join(p_dir, f\"{p_id}_t1ce.nii.gz\")).get_fdata()\n",
    "            t2 = nib.load(os.path.join(p_dir, f\"{p_id}_t2.nii.gz\")).get_fdata()\n",
    "            seg = nib.load(os.path.join(p_dir, f\"{p_id}_seg.nii.gz\")).get_fdata()\n",
    "            \n",
    "            stats['n_samples'] += 1\n",
    "            \n",
    "            # Volume dimensions\n",
    "            H, W, D = flair.shape\n",
    "            stats['volumes']['H'].append(H)\n",
    "            stats['volumes']['W'].append(W)\n",
    "            stats['volumes']['D'].append(D)\n",
    "            \n",
    "            # Class voxel counts\n",
    "            unique, counts = np.unique(seg, return_counts=True)\n",
    "            for u, c in zip(unique, counts):\n",
    "                if u in stats['class_voxels']:\n",
    "                    stats['class_voxels'][int(u)] += c\n",
    "            \n",
    "            # Per-sample class volumes\n",
    "            for cls in [1, 2, 4]:\n",
    "                cls_vol = np.sum(seg == cls)\n",
    "                stats['class_volumes_per_sample'][cls].append(cls_vol)\n",
    "            \n",
    "            # Tumor and brain volumes\n",
    "            tumor_vol = np.sum(seg > 0)\n",
    "            brain_vol = np.sum(flair > 0)\n",
    "            stats['tumor_volumes'].append(tumor_vol)\n",
    "            stats['brain_volumes'].append(brain_vol)\n",
    "            if brain_vol > 0:\n",
    "                stats['tumor_to_brain_ratio'].append(tumor_vol / brain_vol * 100)\n",
    "            \n",
    "            # Intensity statistics (only for non-zero brain region)\n",
    "            brain_mask = flair > 0\n",
    "            \n",
    "            for mod_name, mod_data in [('flair', flair), ('t1', t1), ('t1ce', t1ce), ('t2', t2)]:\n",
    "                brain_voxels = mod_data[brain_mask]\n",
    "                if len(brain_voxels) > 0:\n",
    "                    stats['intensity'][mod_name]['min'].append(brain_voxels.min())\n",
    "                    stats['intensity'][mod_name]['max'].append(brain_voxels.max())\n",
    "                    stats['intensity'][mod_name]['mean'].append(brain_voxels.mean())\n",
    "                    stats['intensity'][mod_name]['std'].append(brain_voxels.std())\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {p_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {stats['n_samples']} samples successfully.\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # GENERATE REPORTS AND PLOTS\n",
    "    # =========================================================================\n",
    "    \n",
    "    # 1. BASIC STATISTICS REPORT\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã DATASET SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüì¶ Total Samples: {stats['n_samples']}\")\n",
    "    \n",
    "    # 2. VOLUME SIZE STATISTICS\n",
    "    print(\"\\nüìê VOLUME DIMENSIONS:\")\n",
    "    for dim, name in [('H', 'Height'), ('W', 'Width'), ('D', 'Depth')]:\n",
    "        arr = np.array(stats['volumes'][dim])\n",
    "        print(f\"   {name}: Min={arr.min()}, Max={arr.max()}, Mean={arr.mean():.1f}, Std={arr.std():.2f}\")\n",
    "    \n",
    "    # 3. CLASS DISTRIBUTION\n",
    "    total_voxels = sum(stats['class_voxels'].values())\n",
    "    print(\"\\nüé® CLASS DISTRIBUTION (Global):\")\n",
    "    class_names = {0: 'Background', 1: 'Necrotic (NCR)', 2: 'Edema (ED)', 4: 'Enhancing (ET)'}\n",
    "    for cls, count in stats['class_voxels'].items():\n",
    "        pct = count / total_voxels * 100\n",
    "        print(f\"   Class {cls} ({class_names[cls]}): {count:,} voxels ({pct:.4f}%)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 1: Volume Dimensions Distribution\n",
    "    # =========================================================================\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for i, (dim, name, color) in enumerate([('H', 'Height', '#3498db'), \n",
    "                                             ('W', 'Width', '#2ecc71'), \n",
    "                                             ('D', 'Depth', '#e74c3c')]):\n",
    "        arr = np.array(stats['volumes'][dim])\n",
    "        axes[i].hist(arr, bins=20, color=color, edgecolor='black', alpha=0.7)\n",
    "        axes[i].axvline(arr.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {arr.mean():.1f}')\n",
    "        axes[i].set_title(f'{name} Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[i].set_xlabel(f'{name} (voxels)')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Volume Dimension Statistics', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"volume_dimensions.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 2: Global Class Distribution (Pie Chart)\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Exclude background for pie chart\n",
    "    tumor_classes = {k: v for k, v in stats['class_voxels'].items() if k != 0}\n",
    "    colors_pie = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "    labels_pie = ['Necrotic (NCR)', 'Edema (ED)', 'Enhancing (ET)']\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        tumor_classes.values(), \n",
    "        labels=labels_pie, \n",
    "        autopct='%1.2f%%',\n",
    "        colors=colors_pie,\n",
    "        explode=(0.02, 0.02, 0.02),\n",
    "        shadow=True,\n",
    "        startangle=90\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Tumor Class Distribution\\n(Excluding Background)', fontsize=16, fontweight='bold')\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"class_distribution_pie.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 3: Class Imbalance Bar Chart\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    class_labels = ['Background', 'Necrotic\\n(NCR)', 'Edema\\n(ED)', 'Enhancing\\n(ET)']\n",
    "    class_values = [stats['class_voxels'][k] for k in [0, 1, 2, 4]]\n",
    "    colors_bar = ['#95a5a6', '#e74c3c', '#2ecc71', '#3498db']\n",
    "    \n",
    "    bars = ax.bar(class_labels, class_values, color=colors_bar, edgecolor='black')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylabel('Voxel Count (Log Scale)', fontsize=12)\n",
    "    ax.set_title('Class Imbalance Visualization\\n(Log Scale)', fontsize=16, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, class_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                f'{val:,.0f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"class_imbalance_bar.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 4: Per-Sample Tumor Volume Distribution\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    tumor_vols = np.array(stats['tumor_volumes'])\n",
    "    ax.hist(tumor_vols, bins=50, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(tumor_vols.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {tumor_vols.mean():,.0f}')\n",
    "    ax.axvline(np.median(tumor_vols), color='orange', linestyle='--', linewidth=2, label=f'Median: {np.median(tumor_vols):,.0f}')\n",
    "    ax.set_xlabel('Total Tumor Volume (voxels)', fontsize=12)\n",
    "    ax.set_ylabel('Number of Patients', fontsize=12)\n",
    "    ax.set_title('Distribution of Total Tumor Volume Across Patients', fontsize=16, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"tumor_volume_distribution.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 5: Per-Class Volume Distribution (Box Plot)\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    box_data = [stats['class_volumes_per_sample'][1], \n",
    "                stats['class_volumes_per_sample'][2], \n",
    "                stats['class_volumes_per_sample'][4]]\n",
    "    \n",
    "    bp = ax.boxplot(box_data, labels=['Necrotic (NCR)', 'Edema (ED)', 'Enhancing (ET)'],\n",
    "                    patch_artist=True)\n",
    "    \n",
    "    colors_box = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel('Voxel Count', fontsize=12)\n",
    "    ax.set_title('Per-Sample Class Volume Distribution', fontsize=16, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"class_volume_boxplot.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 6: Tumor-to-Brain Ratio Distribution\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    ratios = np.array(stats['tumor_to_brain_ratio'])\n",
    "    ax.hist(ratios, bins=50, color='#f39c12', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(ratios.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {ratios.mean():.2f}%')\n",
    "    ax.set_xlabel('Tumor/Brain Ratio (%)', fontsize=12)\n",
    "    ax.set_ylabel('Number of Patients', fontsize=12)\n",
    "    ax.set_title('Tumor-to-Brain Volume Ratio Distribution', fontsize=16, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"tumor_brain_ratio.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 7: Intensity Statistics per Modality\n",
    "    # =========================================================================\n",
    "    modalities = ['flair', 't1', 't1ce', 't2']\n",
    "    mod_labels = ['FLAIR', 'T1', 'T1ce', 'T2']\n",
    "    \n",
    "    # Mean Intensity Distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    colors_mod = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6']\n",
    "    \n",
    "    for i, (mod, label, color) in enumerate(zip(modalities, mod_labels, colors_mod)):\n",
    "        means = np.array(stats['intensity'][mod]['mean'])\n",
    "        axes[i].hist(means, bins=40, color=color, edgecolor='black', alpha=0.7)\n",
    "        axes[i].axvline(means.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                        label=f'Global Mean: {means.mean():.1f}')\n",
    "        axes[i].set_title(f'{label} - Mean Intensity Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Mean Intensity')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Per-Patient Mean Intensity Distribution by Modality', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"intensity_mean_distribution.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 8: Intensity Range (Min-Max) per Modality\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x_pos = np.arange(len(modalities))\n",
    "    width = 0.35\n",
    "    \n",
    "    mins = [np.mean(stats['intensity'][mod]['min']) for mod in modalities]\n",
    "    maxs = [np.mean(stats['intensity'][mod]['max']) for mod in modalities]\n",
    "    \n",
    "    bars1 = ax.bar(x_pos - width/2, mins, width, label='Mean Min', color='#3498db', edgecolor='black')\n",
    "    bars2 = ax.bar(x_pos + width/2, maxs, width, label='Mean Max', color='#e74c3c', edgecolor='black')\n",
    "    \n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(mod_labels)\n",
    "    ax.set_ylabel('Intensity Value', fontsize=12)\n",
    "    ax.set_title('Average Intensity Range per Modality', fontsize=16, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"intensity_range.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 9: Intensity Standard Deviation Comparison\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    std_data = [stats['intensity'][mod]['std'] for mod in modalities]\n",
    "    bp = ax.boxplot(std_data, labels=mod_labels, patch_artist=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], colors_mod):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel('Standard Deviation', fontsize=12)\n",
    "    ax.set_title('Intensity Variability (Std Dev) Across Modalities', fontsize=16, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"intensity_std_boxplot.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOT 10: Correlation Between Tumor Classes\n",
    "    # =========================================================================\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    corr_data = pd.DataFrame({\n",
    "        'Necrotic': stats['class_volumes_per_sample'][1],\n",
    "        'Edema': stats['class_volumes_per_sample'][2],\n",
    "        'Enhancing': stats['class_volumes_per_sample'][4]\n",
    "    })\n",
    "    \n",
    "    corr_matrix = corr_data.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                fmt='.3f', square=True, ax=ax, linewidths=0.5)\n",
    "    ax.set_title('Correlation Between Tumor Class Volumes', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RAWDATA_PATH, \"class_correlation.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SUMMARY TABLE\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä INTENSITY STATISTICS SUMMARY TABLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary_data = []\n",
    "    for mod, label in zip(modalities, mod_labels):\n",
    "        summary_data.append({\n",
    "            'Modality': label,\n",
    "            'Mean Min': np.mean(stats['intensity'][mod]['min']),\n",
    "            'Mean Max': np.mean(stats['intensity'][mod]['max']),\n",
    "            'Mean': np.mean(stats['intensity'][mod]['mean']),\n",
    "            'Std': np.mean(stats['intensity'][mod]['std'])\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False, float_format=\"%.2f\"))\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary_df.to_csv(os.path.join(RAWDATA_PATH, \"intensity_statistics.csv\"), index=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ All statistics and visualizations saved to:\", RAWDATA_PATH)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Execute\n",
    "dataset_stats = calculate_dataset_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What preprocessings needed before using it to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our **BraTSMamba** model learns robust features efficiently, we implement a rigorous preprocessing and augmentation pipeline. This strategy addresses specific challenges inherent to 3D MRI data, such as non-standardized intensity ranges and large volumetric dimensions.\n",
    "\n",
    "#### 1. Preprocessing (Static / Offline)\n",
    "These steps are applied to clean and standardize the raw NIfTI files before they enter the training loop.\n",
    "\n",
    "| Step | Technique | Justification |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Intensity Standardization** | **Z-Score Normalization** | Unlike CT scans (Hounsfield units), MRI signal intensities are relative and vary between scanners. We normalize non-zero voxels (brain tissue) per channel to zero mean and unit variance ($ \\mu=0, \\sigma=1 $) to stabilize gradients. |\n",
    "| **2. Volume Reduction** | **ROI Cropping** | Raw MRI volumes ($240 \\times 240 \\times 155$) contain significant background (\"black void\"). We compute the bounding box of the brain and crop the volume to remove useless zeros, reducing input dimensions for the attention mechanisms. |\n",
    "| **3. Label Mapping** | **Remap {4} $\\to$ {3}** | The BraTS dataset uses labels `{0, 1, 2, 4}`. We remap label `4` (Enhancing Tumor) to `3` to ensure contiguous class indices `{0, 1, 2, 3}` for One-Hot encoding and Loss calculation. |\n",
    "| **4. Memory Optimization** | **Float16 Casting** | We cast input tensors to `float16`. This reduces dataset size by **50%**, minimizing disk I/O latency and VRAM usage without compromising segmentation accuracy. |\n",
    "\n",
    "#### 2. Augmentation (Dynamic / On-the-Fly)\n",
    "Applied during training to prevent overfitting and improve generalization.\n",
    "\n",
    "*   **Random Spatial Crop**: We extract fixed-size patches (e.g., $128^3$) centered around tumor regions (`pos=1`) and background (`neg=1`). This ensures the model sees relevant features while fitting into GPU memory.\n",
    "*   **Random Flips**: Randomly flipping axes (Axial, Sagittal, Coronal) simulates different patient orientations.\n",
    "*   **Intensity Shift**: Randomly shifting intensity values ($\\pm 0.1$) helps the model generalize to slight variations in contrast.\n",
    "\n",
    "#### 3. Validation Protocol\n",
    "For validation, we avoid random augmentations to ensure deterministic evaluation.\n",
    "*   **Divisible Padding**: We pad the input volumes to be divisible by **16** (e.g., $128 \\to 144$). This is critical for U-Net-like architectures where downsampling layers (Pool/Stride) must match upsampling layers perfectly to allow skip connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ† Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "Content :\n",
    "\n",
    "- Motivation & Strategy\n",
    "- Preprocessing & Optimization Protocol\n",
    "- Preprocessing of the BraTS 2021 Dataset\n",
    "  - Dataset Normalization & Standardization\n",
    "  - Visualize some samples after normalization to verify the preprocessing step\n",
    "  - Preprocessing & Augmentation Transforms Class Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation & Strategy\n",
    "\n",
    "In deep learning research, particularly with 3D Volumetric MRI data, the bottleneck is often **I/O throughput** and **VRAM limitations** rather than pure compute power.\n",
    "\n",
    "**Our Approach:**\n",
    "We have decoupled data preprocessing from the model training pipeline. Instead of processing raw NIfTI files on-the-fly during training (which is CPU-intensive and slows down GPU utilization), we create a persistent, optimized dataset.\n",
    "\n",
    "* **Reproducibility:** This creates a frozen \"Gold Standard\" version of the data that ensures consistent inputs across multiple experimental runs.\n",
    "\n",
    "## Preprocessing & Optimization Protocol\n",
    "\n",
    "Every transformation applied in this pipeline serves a specific purpose to maximize model convergence and efficiency.\n",
    "\n",
    "| Optimization Step | Technique | Justification |\n",
    "| --- | --- | --- |\n",
    "| **1. Volume Reduction** | **ROI Cropping** | MRI scans contain significant \"black void\" (background) that contributes zero information. We compute the bounding box of the brain and crop the volume. **Benefit:** Drastically reduces the input dimensions for 3D Attention mechanisms (critical for Transformers/Mamba) without losing anatomical context. |\n",
    "| **2. Signal Standardization** | **Z-Score Normalization** | MRI pixel intensities are relative, not absolute (unlike CT scans). We apply Z-Score normalization per channel, ignoring zero-backgrounds. **Benefit:** Stabilizes gradient descent and accelerates convergence by centering input distributions. |\n",
    "| **3. Precision Optimization** | **Float16 Cast** | Medical images typically do not require 64-bit precision. We cast tensors to `float16`. **Benefit:** Reduces dataset size by **50%** (14GB  7.2GB), doubling the potential batch size during training and reducing PCIe bus latency. |\n",
    "\n",
    "\n",
    "- We generates  `BraTS2021_Samples` file containing processed `.npy` tensors. This artifact will serve as the primary input for the **BraTS-Mamba**, **Swin-UNet**, **3D-Unet** and **UNETR** training experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the BraTS 2021 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we normalize the MRI volumes using Z-Score normalization, focusing only on non-zero voxels (brain tissue) to avoid skewing the statistics with background zeros.\n",
    "\n",
    "- We gonna normalize the whole dataset once and save the preprocessed files to disk for efficient loading during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üì¶ DATASET PREPROCESSING: Z-Score Normalization & Saving\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    \"\"\"\n",
    "    Normalize non-zero voxels using Z-Score normalization.\n",
    "    Only brain tissue (non-zero voxels) is normalized to avoid \n",
    "    skewing statistics with background zeros.\n",
    "    \"\"\"\n",
    "    mask = volume > 0\n",
    "    if mask.sum() > 0:\n",
    "        mean = volume[mask].mean()\n",
    "        std = volume[mask].std()\n",
    "        volume[mask] = (volume[mask] - mean) / (std + 1e-8)\n",
    "    return volume\n",
    "\n",
    "def preprocess_and_save_dataset(all_files, output_dir, device=None):\n",
    "    \"\"\"\n",
    "    Preprocess entire BraTS dataset:\n",
    "    1. Z-Score normalize each modality (non-zero voxels only)\n",
    "    2. Remap label 4 -> 3 for contiguous class indices\n",
    "    3. Save as float16 .npy files for efficient loading\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"üîÑ Preprocessing {len(all_files)} samples...\")\n",
    "    print(f\"üìÇ Output directory: {output_dir}\")\n",
    "    print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for sample in tqdm(all_files, desc=\"Preprocessing\"):\n",
    "        try:\n",
    "            # Extract patient ID from path\n",
    "            p_id = os.path.basename(os.path.dirname(sample[\"image\"][0]))\n",
    "            \n",
    "            # Check if already processed\n",
    "            x_path = os.path.join(output_dir, f\"{p_id}_x.npy\")\n",
    "            y_path = os.path.join(output_dir, f\"{p_id}_y.npy\")\n",
    "            \n",
    "            if os.path.exists(x_path) and os.path.exists(y_path):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Load all 4 modalities: [flair, t1, t1ce, t2]\n",
    "            modalities = []\n",
    "            for img_path in sample[\"image\"]:\n",
    "                vol = nib.load(img_path).get_fdata().astype(np.float32)\n",
    "                modalities.append(vol)\n",
    "            \n",
    "            # Stack modalities: (4, H, W, D)\n",
    "            image_stack = np.stack(modalities, axis=0)\n",
    "            \n",
    "            # Load segmentation mask\n",
    "            mask = nib.load(sample[\"label\"]).get_fdata().astype(np.uint8)\n",
    "            \n",
    "            # Move to GPU for faster processing\n",
    "            image_tensor = torch.from_numpy(image_stack).to(device)\n",
    "            \n",
    "            # Z-Score normalize each modality (non-zero voxels only)\n",
    "            for i in range(4):\n",
    "                channel = image_tensor[i]\n",
    "                nonzero_mask = channel > 0\n",
    "                if nonzero_mask.sum() > 0:\n",
    "                    mean = channel[nonzero_mask].mean()\n",
    "                    std = channel[nonzero_mask].std()\n",
    "                    channel[nonzero_mask] = (channel[nonzero_mask] - mean) / (std + 1e-8)\n",
    "                image_tensor[i] = channel\n",
    "            \n",
    "            # Remap labels: {0, 1, 2, 4} -> {0, 1, 2, 3}\n",
    "            mask[mask == 4] = 3\n",
    "            \n",
    "            # Convert to numpy and cast to float16 for storage efficiency\n",
    "            image_np = image_tensor.cpu().numpy().astype(np.float16)\n",
    "            mask_np = mask.astype(np.uint8)\n",
    "            \n",
    "            # Save preprocessed files\n",
    "            np.save(x_path, image_np)\n",
    "            np.save(y_path, mask_np)\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {sample}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing Complete!\")\n",
    "    print(f\"   Processed: {processed_count}\")\n",
    "    print(f\"   Skipped (already exists): {skipped_count}\")\n",
    "    print(f\"   Output saved to: {output_dir}\")\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "# Execute preprocessing\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Starting Dataset Preprocessing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "preprocess_and_save_dataset(all_files, NORMALIZED_DATA_PATH, device=DEVICE)\n",
    "\n",
    "# Verify output\n",
    "processed_files = gb.glob(os.path.join(NORMALIZED_DATA_PATH, \"*_x.npy\"))\n",
    "print(f\"\\nüìä Verification: Found {len(processed_files)} preprocessed samples in {NORMALIZED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some samples after normalization to verify the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üëÅÔ∏è VISUALIZATION OF NORMALIZED SAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_normalized_samples():\n",
    "    sample_ids = [\"BraTS2021_01498\", \"BraTS2021_00107\", \"BraTS2021_01219\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 6, figsize=(30, 18))\n",
    "    \n",
    "    for row, p_id in enumerate(sample_ids):\n",
    "        x_path = os.path.join(NORMALIZED_DATA_PATH, f\"{p_id}_x.npy\")\n",
    "        y_path = os.path.join(NORMALIZED_DATA_PATH, f\"{p_id}_y.npy\")\n",
    "        \n",
    "        if not os.path.exists(x_path):\n",
    "            print(f\"‚ùå {p_id} not found in processed data!\")\n",
    "            continue\n",
    "            \n",
    "        # Load processed tensors\n",
    "        # Shape: (4, H, W, D)\n",
    "        img_tensor = np.load(x_path)\n",
    "        # Shape: (H, W, D)\n",
    "        seg_tensor = np.load(y_path)\n",
    "        \n",
    "        # Find slice with max tumor area\n",
    "        slice_idx = np.argmax(np.sum(seg_tensor > 0, axis=(0, 1)))\n",
    "        \n",
    "        # Modality names for display\n",
    "        mod_names = [\"FLAIR\", \"T1\", \"T1CE\", \"T2\"]\n",
    "        \n",
    "        # Plot 4 Modalities\n",
    "        for i in range(4):\n",
    "            # Transpose needed because nibabel loads (H, W, D) but imshow expects (H, W)\n",
    "            # Our preprocessing kept the spatial dims consistent with nibabel load\n",
    "            slice_img = img_tensor[i, :, :, slice_idx].T\n",
    "            \n",
    "            axes[row, i].imshow(slice_img, cmap='gray', origin='lower')\n",
    "            axes[row, i].set_title(f\"{mod_names[i]} (Norm)\", fontsize=14)\n",
    "            if i == 0:\n",
    "                axes[row, i].set_ylabel(p_id, fontsize=12, fontweight='bold')\n",
    "            axes[row, i].axis('off')\n",
    "            \n",
    "            # Add text showing min/max to verify normalization\n",
    "            axes[row, i].text(5, 5, f\"Range: [{slice_img.min():.1f}, {slice_img.max():.1f}]\", \n",
    "                             color='yellow', fontsize=10, fontweight='bold')\n",
    "\n",
    "        # 5. Segmentation Mask\n",
    "        mask_slice = seg_tensor[:, :, slice_idx].T\n",
    "        mask_rgb = np.zeros((*mask_slice.shape, 3))\n",
    "        \n",
    "        # Remapped labels: 1=NCR, 2=ED, 3=ET (was 4)\n",
    "        mask_rgb[mask_slice == 1] = [1, 0, 0]  # NCR - Red\n",
    "        mask_rgb[mask_slice == 2] = [0, 1, 0]  # ED - Green\n",
    "        mask_rgb[mask_slice == 3] = [0, 0, 1]  # ET - Blue (Label 3 now)\n",
    "        \n",
    "        axes[row, 4].imshow(mask_rgb, origin='lower')\n",
    "        axes[row, 4].set_title(\"Segmentation\\n(Red=NCR, Green=ED, Blue=ET)\", fontsize=14)\n",
    "        axes[row, 4].axis('off')\n",
    "        \n",
    "        # 6. Overlay (FLAIR + Seg)\n",
    "        flair_slice = img_tensor[0, :, :, slice_idx].T\n",
    "        axes[row, 5].imshow(flair_slice, cmap='gray', origin='lower')\n",
    "        \n",
    "        mask_rgba = np.zeros((*mask_slice.shape, 4))\n",
    "        mask_rgba[mask_slice == 1] = [1, 0, 0, 0.5]\n",
    "        mask_rgba[mask_slice == 2] = [0, 1, 0, 0.5]\n",
    "        mask_rgba[mask_slice == 3] = [0, 0, 1, 0.5]\n",
    "        \n",
    "        axes[row, 5].imshow(mask_rgba, origin='lower')\n",
    "        axes[row, 5].set_title(\"FLAIR + Overlay\", fontsize=14)\n",
    "        axes[row, 5].axis('off')\n",
    "\n",
    "    plt.suptitle(\"Normalized BraTS Samples (Z-Score Verified)\", fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PROCDATARESULTS_PATH, \"normalized_samples_check.png\"), bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "visualize_normalized_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Augmentation Transforms Class Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üîÑ TRANSFORMS\n",
    "# =============================================================================\n",
    "\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "\n",
    "# Training: Aggressive Augmentation + Fixed Cropping\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=0),\n",
    "    EnsureChannelFirstd(keys=[\"label\"], channel_dim=\"no_channel\"),\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    MapLabelValued(keys=[\"label\"], orig_labels=[0, 1, 2, 4], target_labels=[0, 1, 2, 3]),\n",
    "    ScaleIntensityd(keys=[\"image\"]),\n",
    "    # Safety: Pad small images to avoid crash during crop\n",
    "    SpatialPadd(keys=[\"image\", \"label\"], spatial_size=IMG_SIZE, method='symmetric'),\n",
    "    RandCropByPosNegLabeld(\n",
    "        keys=[\"image\", \"label\"], label_key=\"label\",\n",
    "        spatial_size=IMG_SIZE, pos=1, neg=1, num_samples=1,\n",
    "        image_key=\"image\", image_threshold=0,\n",
    "    ),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n",
    "])\n",
    "\n",
    "# Validation: Full Volume (No Crop) + Divisible Padding\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=0),\n",
    "    EnsureChannelFirstd(keys=[\"label\"], channel_dim=\"no_channel\"),\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    MapLabelValued(keys=[\"label\"], orig_labels=[0, 1, 2, 4], target_labels=[0, 1, 2, 3]),\n",
    "    ScaleIntensityd(keys=[\"image\"]),\n",
    "    # Safety 1: Pad to minimum size\n",
    "    SpatialPadd(keys=[\"image\", \"label\"], spatial_size=IMG_SIZE, method='symmetric'),\n",
    "    # Safety 2: Pad to be divisible by 16 (Required for U-Net/Mamba downsampling)\n",
    "    DivisiblePadd(keys=[\"image\", \"label\"], k=16)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß†üêç Model Architecture: BraTSMamba\n",
    "\n",
    "---\n",
    "\n",
    "Content :\n",
    "\n",
    "- Key Design Decisions\n",
    "  - The Dual-Stage CNN Stem (Our Innovation)\n",
    "  - The Mamba Encoder (The Engine)\n",
    "  - Deep Supervision & Skip Connections\n",
    "- Key References\n",
    "- Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Design Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Fusing Local Texture with Global Context}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture, **BratMamba**, addresses the limitations of purely Convolutional networks (like nnU-Net) and purely Transformer-based networks (like Swin UNETR) by leveraging the **Linear Complexity ($O(N)$)** of State Space Models (Mamba).\n",
    "\n",
    "\n",
    "1. **The Dual-Stage CNN Stem (Our Innovation)**\n",
    "      * **The Component**: Instead of a single $7 \\times 7$ patch embedding (like in Swin UNETR), we split the input into two parallel paths: one with a small kernel ($3 \\times 3$) and one with a large kernel ($7 \\times 7$).\n",
    "          * **Stream A ($3 \\times 3$ Kernel)**: Captures high-frequency details (sharp edges of the Necrotic Core).\n",
    "          * **Stream B ($7 \\times 7$ Kernel)**: Captures low-frequency context (large Edema regions).\n",
    "          * **Fusion**: These are concatenated to give the Mamba blocks a \"rich\" feature set that contains both texture and context. \n",
    "      * **The Reason**:\n",
    "          * **Local Texture**: MRI Brain tumor boundaries are defined by subtle texture changes (Necrosis vs. Edema). Small kernels capture these high-frequency edges.\n",
    "          * **Receptive Field**: Large kernels capture the \"neighborhood\" context immediately.\n",
    "          * **Paper Reference**: *Swin UNETR* (Hatamizadeh et al., 2022) highlights the importance of patch merging, but notes that Transformers often lose local spatial details early on. Our Dual-Stem preserves this before the Mamba layers take over.\n",
    "          \n",
    "2. **The Mamba Encoder (The Engine)**\n",
    "      * **The Component**: Stacked Mamba Blocks that flatten the 3D volume into a 1D sequence.\n",
    "      * **The Reason**:\n",
    "          * **The Problem**: Standard Self-Attention (Transformers) scales quadratically ($N^2$). For a 3D volume of $128^3$, the sequence length is ~2 million. A Transformer would run out of memory immediately.\n",
    "          * **The Solution**: Mamba scales linearly ($N$). It allows us to scan the entire 3D brain volume as a single sequence, understanding that \"a pixel at the top left\" is related to \"a pixel at the bottom right\" (Global Context).\n",
    "            * **Global Scanning**: The Mamba block scans the image left-to-right, right-to-left, and top-to-bottom, effectively \"seeing\" the whole brain at once to decide if a pixel is tumor or noise. \n",
    "          * **Paper Reference**: SegMamba (Xing et al., 2024) demonstrates that Mamba outperforms Transformers on 3D medical data by reducing memory usage by 60% while improving Dice scores on the BraTS dataset.\n",
    "\n",
    "3. **Deep Supervision & Skip Connections**\n",
    "    * **The Component**: Direct connections between the Encoder and Decoder at matching resolutions.\n",
    "    * **The Reason**: As the network goes deeper to understand \"shapes\" (Tumor vs Brain), it loses spatial resolution. Skip connections inject the high-resolution texture details from the Stem directly into the Decoder, ensuring the final mask has sharp edges.\n",
    "    * **Paper Reference**: nnU-Net (Isensee et al., 2020) proves that robust encoder-decoder connections are often more important than the choice of optimizer or activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Initialized on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üß† MODEL ARCHITECTURE: BraTSMamba\n",
    "# =============================================================================\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è WARNING: mamba_ssm not found. Using Mock layer (Install for real training!)\")\n",
    "    class Mamba(nn.Module):\n",
    "        def __init__(self, d_model, d_state, d_conv, expand): super().__init__()\n",
    "        def forward(self, x): return x\n",
    "\n",
    "class DualConvStem(nn.Module):\n",
    "    \"\"\"Hybrid Stem: Captures fine details (3x3) and coarse context (7x7).\"\"\"\n",
    "    def __init__(self, in_chans, out_chans):\n",
    "        super().__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv3d(in_chans, out_chans // 2, kernel_size=3, padding=1, stride=2),\n",
    "            nn.InstanceNorm3d(out_chans // 2), nn.GELU()\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv3d(in_chans, out_chans // 2, kernel_size=7, padding=3, stride=2),\n",
    "            nn.InstanceNorm3d(out_chans // 2), nn.GELU()\n",
    "        )\n",
    "        self.fusion = nn.Conv3d(out_chans, out_chans, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fusion(torch.cat([self.branch1(x), self.branch2(x)], dim=1))\n",
    "\n",
    "class MambaLayer(nn.Module):\n",
    "    \"\"\"Volumetric Mamba Block: Flattens 3D -> Sequence -> Mamba -> 3D.\"\"\"\n",
    "    def __init__(self, dim, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mamba = Mamba(d_model=dim, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        x_flat = x.flatten(2).transpose(1, 2)\n",
    "        x_mamba = self.mamba(self.norm(x_flat))\n",
    "        out = x_flat + x_mamba\n",
    "        return out.transpose(1, 2).view(B, C, D, H, W)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"Standard U-Net Decoder Block.\"\"\"\n",
    "    def __init__(self, in_chans, out_chans):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose3d(in_chans, out_chans, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(out_chans * 2, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm3d(out_chans), nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape != skip.shape: # Handle padding mismatches\n",
    "            x = nn.functional.interpolate(x, size=skip.shape[2:], mode='trilinear')\n",
    "        return self.conv(torch.cat([x, skip], dim=1))\n",
    "\n",
    "class BraTSMamba(nn.Module):\n",
    "    def __init__(self, in_chans=4, num_classes=4, embed_dim=48):\n",
    "        super().__init__()\n",
    "        self.stem = DualConvStem(in_chans, embed_dim)\n",
    "        self.layer1 = MambaLayer(embed_dim)\n",
    "        self.down1 = nn.Conv3d(embed_dim, embed_dim*2, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer2 = MambaLayer(embed_dim*2)\n",
    "        self.down2 = nn.Conv3d(embed_dim*2, embed_dim*4, kernel_size=3, stride=2, padding=1)\n",
    "        self.bottleneck = MambaLayer(embed_dim*4)\n",
    "        \n",
    "        self.up1 = UpBlock(embed_dim*4, embed_dim*2)\n",
    "        self.up2 = UpBlock(embed_dim*2, embed_dim)\n",
    "        self.final_up = nn.ConvTranspose3d(embed_dim, embed_dim, kernel_size=2, stride=2)\n",
    "        self.out_head = nn.Conv3d(embed_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(self.stem(x))\n",
    "        x2 = self.layer2(self.down1(x1))\n",
    "        x3 = self.bottleneck(self.down2(x2))\n",
    "        d1 = self.up1(x3, x2)\n",
    "        d2 = self.up2(d1, x1)\n",
    "        return self.out_head(self.final_up(d2))\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BraTSMamba().to(DEVICE)\n",
    "print(f\"‚úÖ Model Initialized on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  üèãüèæüèÉüèæ‚Äç‚û°Ô∏èüöµüèæBraTSMamba Training\n",
    "\n",
    "---\n",
    "\n",
    "Content :\n",
    "\n",
    "-  Model Initialization Strategy\n",
    "-  Validation Approach\n",
    "-  Hyperparameter Justification\n",
    "-  Configurations for BraTSMamba Model Training\n",
    "-  Setting up Data Loaders\n",
    "-  Visualze Fully processed & normalized samples\n",
    "-  Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model Initialization Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declaration:** **Random Initialization (Training from Scratch)**\n",
    "\n",
    "We explicitly state that **no pretrained weights** were used for the **BraTSMamba** model. \n",
    "*   **Weight Usage:** All weights are initialized randomly using PyTorch's default initialization (Kaiming/Xavier where applicable).\n",
    "*   **Trainable Layers:** **100%** of the layers are fully trainable. No layers are frozen.\n",
    "*   **Justification:** Since we are introducing a novel hybrid architecture (Dual-Stem + Mamba), standard pretrained weights (e.g., from ImageNet 2D) are not directly compatible with our 3D Volumetric inputs and specific channel dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategy:** **Hold-out Validation Split (80/20)**\n",
    "*   **Method:** We utilize a deterministic 80% Train / 20% Validation split based on sorted file lists.\n",
    "*   **Justification:** Due to the high computational cost of 3D volumetric training (high VRAM usage and long epoch times), K-Fold cross-validation was computationally infeasible within the hackathon timeframe. A fixed seed (`SEED=42`) ensures the split is reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Hyperparameter | Value | Justification |\n",
    "| :--- | :--- | :--- |\n",
    "| **Batch Size** | `4` (Train) | Limited by GPU VRAM (16GB T4/P100). 3D volumes are memory intensive. We use Gradient Accumulation (implicitly via scaler) to stabilize updates. |\n",
    "| **Batch Size** | `1` (Val) | **Critical:** Validation images have variable sizes. Batch size must be 1 to avoid shape mismatch errors during inference. |\n",
    "| **Learning Rate** | `3e-4` | The \"Karpathy Constant\". A standard starting point for Adam-based optimizers that balances convergence speed and stability for Transformers/SSMs. |\n",
    "| **Optimizer** | `AdamW` | Chosen over SGD for its ability to handle sparse gradients and decoupled weight decay (`1e-5`), which prevents overfitting in large parameter models like Mamba. |\n",
    "| **Scheduler** | `CosineAnnealing` | Gradually reduces LR to zero. This helps the model settle into a local minimum better than a constant LR. |\n",
    "| **Loss Function** | `DiceCELoss` | **Compound Loss:** Combines Dice Loss (handles class imbalance well) with Cross Entropy (smooth gradients). Essential for small tumor regions. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations for BraTSMamba Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ‚öôÔ∏è CONFIGURATION FOR BRATSMAMBA TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset Limits (Set N_SAMPLES to None for full dataset)\n",
    "N_SAMPLES = None     \n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# Training Hyperparameters for BraTSMamba\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE_TRAIN = 4   # Effective batch size = 8 (on 2 GPUs)\n",
    "BATCH_SIZE_VAL = 1     # MUST be 1 to handle variable image sizes\n",
    "NUM_WORKERS = 8        # High worker count for efficient lazy loading\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_RUNTIME = 12 * 3600 \n",
    "\n",
    "\n",
    "# results of BratsMamba\n",
    "CHECKPOINT_DIR = os.path.join(ARTIFACTS_DIR,\"BraTSMamba\", \"checkpoints\")\n",
    "RESULTS_DIR = os.path.join(ARTIFACTS_DIR,\"BraTSMamba\", \"results\")\n",
    "BEST_MODEL_DIR = os.path.join(ARTIFACTS_DIR,\"BraTSMamba\", \"best_model\")\n",
    "\n",
    "# Create Directories\n",
    "for d in [EXTRACT_PATH, CHECKPOINT_DIR, RESULTS_DIR, BEST_MODEL_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Initializing Loaders (Lazy Loading)...\")\n",
    "# Split Data\n",
    "val_count = int(len(all_files) * VAL_SPLIT)\n",
    "val_count = max(1, val_count) # Ensure at least 1 val sample\n",
    "train_files, val_files = all_files[val_count:], all_files[:val_count]\n",
    "\n",
    "print(f\"üìä Dataset Split: Train={len(train_files)} | Val={len(val_files)}\")\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE_TRAIN, shuffle=True, \n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "# Note: Batch Size 1 is critical for Val to handle variable volume sizes\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE_VAL, shuffle=False, \n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "print(\"‚úÖ Loaders Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualze Fully processed & normalized samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üëÅÔ∏è DATA SANITY CHECK (Compare with Original Sample as well)\n",
    "# =============================================================================\n",
    "def visualize_batch(loader, save_path=None):\n",
    "    print(\"‚è≥ Fetching batch for visualization...\")\n",
    "    try:\n",
    "        batch = next(iter(loader))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching batch: {e}\"); return\n",
    "\n",
    "    images, masks = batch[\"image\"], batch[\"label\"]\n",
    "    print(f\"   Batch Shape: {images.shape}\")\n",
    "    \n",
    "    # Search for a sample with tumor content\n",
    "    sample_idx = 0\n",
    "    for i in range(len(images)):\n",
    "        if masks[i].sum() > 0:\n",
    "            sample_idx = i; break\n",
    "    \n",
    "    img_t = images[sample_idx]\n",
    "    msk_t = masks[sample_idx]\n",
    "    if msk_t.ndim == 4: msk_t = msk_t[0] # Handle channel dim\n",
    "    \n",
    "    # Find Axial Slice with Max Tumor Area\n",
    "    tumor_counts = torch.sum(msk_t > 0, dim=(1, 2))\n",
    "    slice_idx = torch.argmax(tumor_counts).item()\n",
    "    if tumor_counts.max() == 0: slice_idx = img_t.shape[1] // 2\n",
    "    \n",
    "    print(f\"‚úÖ Visualizing Sample {sample_idx}, Slice {slice_idx}\")\n",
    "\n",
    "    # Plot\n",
    "    slice_img = img_t[:, slice_idx, :, :].cpu().numpy()\n",
    "    slice_msk = msk_t[slice_idx, :, :].cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    modes = [\"T1\", \"T1ce\", \"T2\", \"FLAIR\", \"Mask\"]\n",
    "    for i in range(4):\n",
    "        ax[i].imshow(slice_img[i], cmap=\"gray\")\n",
    "        ax[i].set_title(modes[i]); ax[i].axis(\"off\")\n",
    "    \n",
    "    ax[4].imshow(slice_msk, cmap=\"jet\", vmin=0, vmax=3)\n",
    "    ax[4].set_title(\"Ground Truth\"); ax[4].axis(\"off\")\n",
    "    \n",
    "    if save_path: plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "visualize_batch(train_loader, save_path=os.path.join(PROCDATARESULTS_PATH, \"sanity_check.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple GPUs detected; using device 0 only.\n",
      "üöÄ Training starting at Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:18<00:00,  1.27it/s, loss=0.9723]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=1.2345 | Val=0.9531 | Dice=0.1766\n",
      "   ‚≠ê New Best! 0.0000 -> 0.1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:21<00:00,  1.24it/s, loss=0.7045]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:55<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.8538 | Val=0.8307 | Dice=0.3270\n",
      "   ‚≠ê New Best! 0.1766 -> 0.3270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:11<00:00,  1.31it/s, loss=0.7686]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:54<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.6942 | Val=0.7982 | Dice=0.3170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:22<00:00,  1.24it/s, loss=0.3868]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.5240 | Val=0.5695 | Dice=0.5142\n",
      "   ‚≠ê New Best! 0.3270 -> 0.5142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:21<00:00,  1.25it/s, loss=0.3406]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:56<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.4648 | Val=0.4039 | Dice=0.6709\n",
      "   ‚≠ê New Best! 0.5142 -> 0.6709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:22<00:00,  1.24it/s, loss=0.4739]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:55<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.4303 | Val=0.4324 | Dice=0.6279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:18<00:00,  1.27it/s, loss=0.3118]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.4140 | Val=0.4190 | Dice=0.6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:24<00:00,  1.23it/s, loss=0.2681]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.4096 | Val=0.5124 | Dice=0.5472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:16<00:00,  1.28it/s, loss=0.5683]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.4023 | Val=0.4362 | Dice=0.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:14<00:00,  1.29it/s, loss=0.4802]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3852 | Val=0.4678 | Dice=0.5925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:19<00:00,  1.26it/s, loss=0.9034]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:56<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3764 | Val=0.3369 | Dice=0.7058\n",
      "   ‚≠ê New Best! 0.6709 -> 0.7058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:23<00:00,  1.23it/s, loss=0.1613]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:54<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3663 | Val=0.3256 | Dice=0.7168\n",
      "   ‚≠ê New Best! 0.7058 -> 0.7168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:22<00:00,  1.24it/s, loss=0.1940]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3668 | Val=0.3745 | Dice=0.6709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:14<00:00,  1.29it/s, loss=0.1612]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:56<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3515 | Val=0.3482 | Dice=0.7050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:15<00:00,  1.28it/s, loss=0.2495]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3495 | Val=0.2961 | Dice=0.7439\n",
      "   ‚≠ê New Best! 0.7168 -> 0.7439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:13<00:00,  1.29it/s, loss=0.1959]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:55<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3412 | Val=0.2961 | Dice=0.7481\n",
      "   ‚≠ê New Best! 0.7439 -> 0.7481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:14<00:00,  1.29it/s, loss=0.3609]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3436 | Val=0.3011 | Dice=0.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:22<00:00,  1.24it/s, loss=0.3219]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:59<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3309 | Val=0.2889 | Dice=0.7576\n",
      "   ‚≠ê New Best! 0.7481 -> 0.7576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:15<00:00,  1.28it/s, loss=0.3904]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3215 | Val=0.2957 | Dice=0.7529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:16<00:00,  1.28it/s, loss=0.2706]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:56<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3197 | Val=0.2812 | Dice=0.7593\n",
      "   ‚≠ê New Best! 0.7576 -> 0.7593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:22<00:00,  1.24it/s, loss=0.2605]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3094 | Val=0.2894 | Dice=0.7484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:27<00:00,  1.21it/s, loss=0.1221]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3042 | Val=0.2942 | Dice=0.7515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:26<00:00,  1.21it/s, loss=0.3500]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:56<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3142 | Val=0.2929 | Dice=0.7537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:24<00:00,  1.22it/s, loss=0.7892]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3217 | Val=0.2795 | Dice=0.7636\n",
      "   ‚≠ê New Best! 0.7593 -> 0.7636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:19<00:00,  1.26it/s, loss=0.0603]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:56<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3070 | Val=0.2863 | Dice=0.7637\n",
      "   ‚≠ê New Best! 0.7636 -> 0.7637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:25<00:00,  1.22it/s, loss=1.0025]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3076 | Val=0.2801 | Dice=0.7687\n",
      "   ‚≠ê New Best! 0.7637 -> 0.7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:20<00:00,  1.25it/s, loss=0.1069]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:03<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2981 | Val=0.2776 | Dice=0.7620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:18<00:00,  1.26it/s, loss=0.2491]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2971 | Val=0.2979 | Dice=0.7467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [03:23<00:00,  1.24it/s, loss=0.2282]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:57<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3019 | Val=0.3037 | Dice=0.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:59<00:00,  1.40it/s, loss=0.1695]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:45<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2971 | Val=0.2751 | Dice=0.7658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:41<00:00,  1.56it/s, loss=0.4774]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:43<00:00,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2897 | Val=0.2883 | Dice=0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:37<00:00,  1.59it/s, loss=0.2044]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:44<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.3041 | Val=0.2628 | Dice=0.7742\n",
      "   ‚≠ê New Best! 0.7687 -> 0.7742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:39<00:00,  1.58it/s, loss=0.1389]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:44<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2895 | Val=0.2662 | Dice=0.7783\n",
      "   ‚≠ê New Best! 0.7742 -> 0.7783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:39<00:00,  1.57it/s, loss=0.4042]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:43<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2952 | Val=0.2524 | Dice=0.7874\n",
      "   ‚≠ê New Best! 0.7783 -> 0.7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:41<00:00,  1.56it/s, loss=0.3906]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:46<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2722 | Val=0.2699 | Dice=0.7684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:42<00:00,  1.55it/s, loss=0.0675]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:43<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2847 | Val=0.2795 | Dice=0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:40<00:00,  1.57it/s, loss=0.1905]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:43<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2776 | Val=0.2647 | Dice=0.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:39<00:00,  1.57it/s, loss=0.2489]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:44<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2849 | Val=0.2751 | Dice=0.7592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:38<00:00,  1.58it/s, loss=0.0644]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:45<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2855 | Val=0.2542 | Dice=0.7848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:38<00:00,  1.58it/s, loss=0.4496]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:46<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2796 | Val=0.2521 | Dice=0.7880\n",
      "   ‚≠ê New Best! 0.7874 -> 0.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:39<00:00,  1.57it/s, loss=0.1177]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:46<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2825 | Val=0.2423 | Dice=0.7935\n",
      "   ‚≠ê New Best! 0.7880 -> 0.7935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:41<00:00,  1.55it/s, loss=0.0924]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:49<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2735 | Val=0.2508 | Dice=0.7847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:38<00:00,  1.58it/s, loss=0.8235]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:44<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2773 | Val=0.2542 | Dice=0.7813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:40<00:00,  1.56it/s, loss=0.1454]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:43<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2792 | Val=0.2452 | Dice=0.7907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:40<00:00,  1.56it/s, loss=0.7620]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:43<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2727 | Val=0.2471 | Dice=0.7889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:40<00:00,  1.57it/s, loss=0.3730]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:43<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2757 | Val=0.2497 | Dice=0.7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:40<00:00,  1.56it/s, loss=0.1195]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:44<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2717 | Val=0.2485 | Dice=0.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:42<00:00,  1.55it/s, loss=0.1152]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:44<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2825 | Val=0.2482 | Dice=0.7877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:37<00:00,  1.60it/s, loss=0.6901]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:46<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2772 | Val=0.2473 | Dice=0.7887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [02:41<00:00,  1.55it/s, loss=0.2326]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:51<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stats: Train=0.2730 | Val=0.2474 | Dice=0.7886\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üöÄ TRAINING LOOP\n",
    "# =============================================================================\n",
    "GPU_COUNT = torch.cuda.device_count()\n",
    "if GPU_COUNT > 1:\n",
    "    print(\"Multiple GPUs detected; using device 0 only.\")\n",
    "\n",
    "criterion = DiceCELoss(to_onehot_y=True, softmax=True, include_background=False)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda')) \n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "# Resume Logic\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_dice': [], 'epochs': []}\n",
    "start_epoch, best_dice = 0, 0.0\n",
    "last_ckpt = os.path.join(CHECKPOINT_DIR, \"last.pth\")\n",
    "history_path = os.path.join(RESULTS_DIR, \"log.json\")\n",
    "\n",
    "if os.path.exists(last_ckpt):\n",
    "    print(\"üîÑ Resuming from checkpoint...\")\n",
    "    ckpt = torch.load(last_ckpt)\n",
    "    \n",
    "    # Load State Dict handling DataParallel wrapper\n",
    "    sd = ckpt['model_state_dict']\n",
    "    if isinstance(model, nn.DataParallel): model.module.load_state_dict(sd)\n",
    "    else: \n",
    "        # Fix if resuming non-parallel on parallel or vice-versa\n",
    "        new_sd = {k.replace('module.', ''): v for k, v in sd.items()}\n",
    "        model.load_state_dict(new_sd, strict=False)\n",
    "        \n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    scaler.load_state_dict(ckpt['scaler_state_dict'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    best_dice = ckpt['best_dice']\n",
    "    if os.path.exists(history_path):\n",
    "        with open(history_path, 'r') as f: history = json.load(f)\n",
    "\n",
    "print(f\"üöÄ Training starting at Epoch {start_epoch+1}...\")\n",
    "START_TIME = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    if time.time() - START_TIME > MAX_RUNTIME:\n",
    "        print(\"üõë Time limit reached.\"); break\n",
    "        \n",
    "    model.train()\n",
    "    ep_loss = 0\n",
    "    \n",
    "    # Train Step\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Ep {epoch+1}\")\n",
    "    for i, batch in pbar:\n",
    "        img, lbl = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
    "        \n",
    "        # 1. Clear Gradients (Optimizer update preparation)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward Pass (How inputs produce predictions)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "            pred = model(img)\n",
    "            # 3. Loss Computation (How errors are calculated)\n",
    "            loss = criterion(pred, lbl)\n",
    "        \n",
    "        # 4. Backpropagation (How gradients are computed)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        #5. Optimizer Update (How model weights are updated)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        ep_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Val Step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Val\"):\n",
    "            img, lbl = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
    "            with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "                pred = model(img)\n",
    "                val_loss += criterion(pred, lbl).item()\n",
    "            \n",
    "            p = [AsDiscrete(argmax=True, to_onehot=4)(i) for i in pred]\n",
    "            t = [AsDiscrete(to_onehot=4)(i) for i in lbl]\n",
    "            dice_metric(y_pred=p, y=t)\n",
    "            \n",
    "    # Stats\n",
    "    stats = {\n",
    "        'train_loss': ep_loss / len(train_loader),\n",
    "        'val_loss': val_loss / len(val_loader),\n",
    "        'val_dice': dice_metric.aggregate().item()\n",
    "    }\n",
    "    dice_metric.reset()\n",
    "    \n",
    "    # Update History & Log\n",
    "    history['epochs'].append(epoch+1)\n",
    "    for k, v in stats.items(): history[k].append(v)\n",
    "    with open(history_path, 'w') as f: json.dump(history, f)\n",
    "    \n",
    "    print(f\"   Stats: Train={stats['train_loss']:.4f} | Val={stats['val_loss']:.4f} | Dice={stats['val_dice']:.4f}\")\n",
    "    \n",
    "    # Save State\n",
    "    state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    ckpt = {\n",
    "        'epoch': epoch, 'model_state_dict': state,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(), 'best_dice': best_dice\n",
    "    }\n",
    "    torch.save(ckpt, last_ckpt)\n",
    "    \n",
    "    if stats['val_dice'] > best_dice:\n",
    "        print(f\"   ‚≠ê New Best! {best_dice:.4f} -> {stats['val_dice']:.4f}\")\n",
    "        best_dice = stats['val_dice']\n",
    "        torch.save(state, os.path.join(BEST_MODEL_DIR, \"best_model.pth\"))\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1); plt.plot(history['train_loss'], label='Train'); plt.plot(history['val_loss'], label='Val'); plt.legend()\n",
    "plt.subplot(1, 2, 2); plt.plot(history['val_dice'], label='Dice', color='green'); plt.legend()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"training_curves.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù BraTSMamba Evaluation \n",
    "\n",
    "---\n",
    "\n",
    "Content :\n",
    "\n",
    "- Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Best Model: /storage2/ChangeDetection/NSST-mamba/mamba_decoder/UrbanMamba/outputs/bratsmamba/best_model/best_model.pth\n",
      "üîç Starting Final Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [02:53<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ FINAL SCORES:\n",
      "              Region  Dice (DSC) ‚Üë  HD95 (mm) ‚Üì  Mean Dice\n",
      "    Whole Tumor (WT)        0.8933       9.1794     0.8706\n",
      "     Tumor Core (TC)        0.8742       8.6031     0.8706\n",
      "Enhancing Tumor (ET)        0.8443       7.0057     0.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üèÜ EVALUATION & METRICS\n",
    "# =============================================================================\n",
    "def get_brats_regions(tensor_oh):\n",
    "    \"\"\"Converts One-Hot (BG, NCR, ED, ET) -> (WT, TC, ET)\"\"\"\n",
    "    wt = torch.sum(tensor_oh[:, 1:4, ...], dim=1, keepdim=True) > 0\n",
    "    tc = (tensor_oh[:, 1:2, ...] + tensor_oh[:, 3:4, ...]) > 0\n",
    "    et = tensor_oh[:, 3:4, ...] > 0\n",
    "    return torch.cat([wt, tc, et], dim=1).float()\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "    hd95_metric = HausdorffDistanceMetric(include_background=True, percentile=95, reduction=\"mean_batch\")\n",
    "    post_pred = AsDiscrete(argmax=True, to_onehot=4)\n",
    "    post_label = AsDiscrete(to_onehot=4)\n",
    "    \n",
    "    print(\"üîç Starting Final Evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "            img, lbl = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
    "            pred = model(img)\n",
    "            \n",
    "            # Post-process\n",
    "            pred_oh = torch.stack([post_pred(x) for x in pred])\n",
    "            lbl_oh = torch.stack([post_label(x) for x in lbl])\n",
    "            \n",
    "            # Convert to BraTS Regions\n",
    "            pred_reg = get_brats_regions(pred_oh)\n",
    "            lbl_reg = get_brats_regions(lbl_oh)\n",
    "            \n",
    "            dice_metric(y_pred=pred_reg, y=lbl_reg)\n",
    "            hd95_metric(y_pred=pred_reg, y=lbl_reg)\n",
    "            \n",
    "            # Visualize First Batch\n",
    "            if i == 0:\n",
    "                visualize_prediction(img[0], lbl[0], pred[0], \n",
    "                                     os.path.join(RESULTS_DIR, \"best_pred.png\"))\n",
    "\n",
    "    # Aggregate\n",
    "    dice = dice_metric.aggregate().cpu().numpy()\n",
    "    hd95 = hd95_metric.aggregate().cpu().numpy()\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"Region\": [\"Whole Tumor (WT)\", \"Tumor Core (TC)\", \"Enhancing Tumor (ET)\"],\n",
    "        \"Dice (DSC) ‚Üë\": dice,\n",
    "        \"HD95 (mm) ‚Üì\": hd95,\n",
    "        \"Mean Dice\": [dice.mean()] * 3\n",
    "    })\n",
    "    \n",
    "    csv_path = os.path.join(RESULTS_DIR, \"final_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"\\nüèÜ FINAL SCORES:\"); print(df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "def visualize_prediction(img, lbl, pred, save_path):\n",
    "    \"\"\"Helper to visualize input vs prediction.\"\"\"\n",
    "    vol = lbl.sum(dim=(0,1,2)); idx = torch.argmax(vol).item()\n",
    "    if vol.max() == 0: idx = img.shape[2] // 2\n",
    "    \n",
    "    im = img[2, :, :, idx].cpu().numpy()\n",
    "    gt = torch.argmax(lbl, dim=0)[:, :, idx].cpu().numpy()\n",
    "    pr = torch.argmax(pred, dim=0)[:, :, idx].cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(im, cmap='gray'); ax[0].set_title(\"Input (T2)\")\n",
    "    ax[1].imshow(gt, cmap='jet', vmin=0, vmax=3); ax[1].set_title(\"GT\")\n",
    "    ax[2].imshow(pr, cmap='jet', vmin=0, vmax=3); ax[2].set_title(\"Pred\")\n",
    "    plt.savefig(save_path); plt.close()\n",
    "\n",
    "# Load Best & Run\n",
    "best_path = os.path.join(BEST_MODEL_DIR, \"best_model.pth\")\n",
    "if os.path.exists(best_path):\n",
    "    print(f\"üìÇ Loading Best Model: {best_path}\")\n",
    "    sd = torch.load(best_path)\n",
    "    if isinstance(model, nn.DataParallel): model.module.load_state_dict(sd)\n",
    "    else: model.load_state_dict(sd)\n",
    "    evaluate_model(model, val_loader)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No best model found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Benchmark Model Training\n",
    "\n",
    "---\n",
    "\n",
    "Content :\n",
    "\n",
    "- Benchmark Models Overview\n",
    "- Referred Sources\n",
    "- Trainer Function\n",
    "- 3D U-Net Training\n",
    "  - Configurations\n",
    "  - Data Loaders\n",
    "  - Training Loop\n",
    "- UNETR Training\n",
    "  - Configurations\n",
    "  - Data Loaders\n",
    "  - Training Loop\n",
    "- Swin UNETR Training\n",
    "  - Configurations\n",
    "  - Data Loaders\n",
    "  - Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmarking script is designed . It re-initializes the data loaders using the robust method we just fixed, then trains and evaluates three distinct architectures: 3D U-Net (nnU-Net analog), UNETR (Transformer baseline), and Swin UNETR.\n",
    "\n",
    "Benchmarking Strategy\n",
    "\n",
    "* **3D U-Net (nnU-Net Analog)**: The gold standard CNN baseline. We use a residual 3D U-Net configuration similar to the self-configuring method described in [1].\n",
    "\n",
    "\n",
    "* **UNETR**: A standard Vision Transformer (ViT) encoder with a CNN decoder, serving as our \"Analogous Transformer\" baseline.[2]\n",
    "\n",
    "\n",
    "* **Swin UNETR**: A hierarchical Swin Transformer encoder that addresses the fixed-scale limitations of standard ViTs, as detailed in Hatamizadeh et al.[3]\n",
    "\n",
    "> Note: SegMamba is excluded to avoid the heavy installation overhead of mamba-ssm(This also a SOTA mamba architecture, but due to importing overhead we decided to benchmark with simple CNN, and Transfomer methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referred Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[1]O. I√ßek, A. Abdulkadir, S. Lienkamp, T. Brox, and O. Ronneberger, ‚Äú3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation.‚Äù Available: https://arxiv.org/pdf/1606.06650\n",
    "\n",
    "[2]A. Nvidia et al., ‚ÄúUNETR: Transformers for 3D Medical Image Segmentation.‚Äù Available: https://arxiv.org/pdf/2103.10504\n",
    "\n",
    "[3]A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. Roth, and D. Xu, ‚ÄúSwin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images.‚Äù Available: https://arxiv.org/pdf/2201.01266\n",
    "‚Äå\n",
    "‚Äå\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üöÄ BENCHMARK TRAINING ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "# benchmark model path\n",
    "BENCH_PATH = os.path.join(ARTIFACTS_DIR, \"benchmarks\")\n",
    "if not os.path.exists(BENCH_PATH):\n",
    "    os.makedirs(BENCH_PATH)\n",
    "\n",
    "def train_benchmark_model(model_name, model, train_loader, val_loader, device):\n",
    "    print(f\"\\nüöÄ STARTING BENCHMARK: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    save_path = os.path.join(BENCH_PATH, f\"{model_name}_best.pth\")\n",
    "    criterion = DiceCELoss(to_onehot_y=True, softmax=True, include_background=False)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    \n",
    "    scaler = GradScaler('cuda')\n",
    "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "    \n",
    "    best_dice = 0.0\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Training Step\n",
    "        pbar = tqdm(train_loader, desc=f\"{model_name} Ep {epoch+1}/{NUM_EPOCHS}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            imgs, lbls = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, lbls)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "        # Validation Step (Sliding Window)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, lbls = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                with autocast('cuda'):\n",
    "                    # Sliding window is mandatory for Batch Size 1 variable inputs\n",
    "                    preds = sliding_window_inference(imgs, IMG_SIZE, 4, model)\n",
    "                \n",
    "                p = [AsDiscrete(argmax=True, to_onehot=4)(i) for i in preds]\n",
    "                t = [AsDiscrete(to_onehot=4)(i) for i in lbls]\n",
    "                dice_metric(y_pred=p, y=t)\n",
    "        \n",
    "        # Stats\n",
    "        avg_dice = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "        \n",
    "        print(f\"   Ep {epoch+1}: Loss={train_loss/len(train_loader):.4f} | Val Dice={avg_dice:.4f}\")\n",
    "        \n",
    "        # Save Best\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"   ‚≠ê New Best Saved!\")\n",
    "            \n",
    "    print(f\"‚úÖ Finished {model_name}. Best Dice: {best_dice:.4f}\\n\")\n",
    "    return best_dice, save_path\n",
    "\n",
    "# --- RUN EXPERIMENTS ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D U-Net Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs for 3D U-Net\n",
    "\n",
    "# Dataset Limits (Set N_SAMPLES to None for full dataset)\n",
    "N_SAMPLES_3DUNET = None     \n",
    "VAL_SPLIT_3DUNET = 0.2\n",
    "\n",
    "# Training Hyperparameters for BraTSMamba\n",
    "IMG_SIZE_3DUNET = (128, 128, 128)\n",
    "BATCH_SIZE_TRAIN_3DUNET = 4   # Effective batch size = 8 (on 2 GPUs)\n",
    "BATCH_SIZE_VAL_3DUNET = 1     # MUST be 1 to handle variable image sizes\n",
    "NUM_WORKERS_3DUNET = 8        # High worker count for efficient lazy loading\n",
    "NUM_EPOCHS_3DUNET = 50\n",
    "LEARNING_RATE_3DUNET = 3e-4\n",
    "MAX_RUNTIME_3DUNET = 12 * 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Initializing Loaders (Lazy Loading)...\")\n",
    "# Split Data\n",
    "val_count = int(len(all_files) * VAL_SPLIT_3DUNET)\n",
    "val_count = max(1, val_count) # Ensure at least 1 val sample\n",
    "train_files, val_files = all_files[val_count:], all_files[:val_count]\n",
    "\n",
    "print(f\"üìä Dataset Split: Train={len(train_files)} | Val={len(val_files)}\")\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE_TRAIN_3DUNET, shuffle=True, \n",
    "    num_workers=NUM_WORKERS_3DUNET, pin_memory=True\n",
    ")\n",
    "# Note: Batch Size 1 is critical for Val to handle variable volume sizes\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE_VAL_3DUNET, shuffle=False, \n",
    "    num_workers=NUM_WORKERS_3DUNET, pin_memory=True\n",
    ")\n",
    "print(\"‚úÖ Loaders Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 3D U-Net (Baseline)\n",
    "model_unet = UNet(\n",
    "    spatial_dims=3, in_channels=4, out_channels=4, \n",
    "    channels=(32, 64, 128, 256, 512), strides=(2, 2, 2, 2), \n",
    "    num_res_units=2, norm=\"instance\"\n",
    ").to(DEVICE)\n",
    "d_unet, p_unet = train_benchmark_model(\"3D_UNet\", model_unet, train_loader, val_loader, DEVICE)\n",
    "results[\"3D_UNet\"] = {\"Path\": p_unet, \"Paper\": \"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation.\"}\n",
    "del model_unet; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNETR Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs for 3D U-Net\n",
    "\n",
    "# Dataset Limits (Set N_SAMPLES to None for full dataset)\n",
    "N_SAMPLES_UNETR = None     \n",
    "VAL_SPLIT_UNETR = 0.2\n",
    "\n",
    "# Training Hyperparameters for BraTSMamba\n",
    "IMG_SIZE_UNETR = (128, 128, 128)\n",
    "BATCH_SIZE_TRAIN_UNETR = 4   # Effective batch size = 8 (on 2 GPUs)\n",
    "BATCH_SIZE_VAL_UNETR = 1     # MUST be 1 to handle variable image sizes\n",
    "NUM_WORKERS_UNETR = 8        # High worker count for efficient lazy loading\n",
    "NUM_EPOCHS_UNETR = 50\n",
    "LEARNING_RATE_UNETR = 3e-4\n",
    "MAX_RUNTIME_UNETR = 12 * 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Initializing Loaders (Lazy Loading)...\")\n",
    "# Split Data\n",
    "val_count = int(len(all_files) * VAL_SPLIT_UNETR)\n",
    "val_count = max(1, val_count) # Ensure at least 1 val sample\n",
    "train_files, val_files = all_files[val_count:], all_files[:val_count]\n",
    "\n",
    "print(f\"üìä Dataset Split: Train={len(train_files)} | Val={len(val_files)}\")\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE_TRAIN_UNETR, shuffle=True, \n",
    "    num_workers=NUM_WORKERS_UNETR, pin_memory=True\n",
    ")\n",
    "# Note: Batch Size 1 is critical for Val to handle variable volume sizes\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE_VAL_UNETR, shuffle=False, \n",
    "    num_workers=NUM_WORKERS_UNETR, pin_memory=True\n",
    ")\n",
    "print(\"‚úÖ Loaders Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. UNETR (Transformer)\n",
    "model_unetr = UNETR(\n",
    "    in_channels=4, \n",
    "    out_channels=4, \n",
    "    img_size=Config.IMG_SIZE, \n",
    "    feature_size=16, \n",
    "    hidden_size=768, \n",
    "    mlp_dim=3072, \n",
    "    num_heads=12, \n",
    "    norm_name=\"instance\", \n",
    "    res_block=True\n",
    ").to(DEVICE)\n",
    "\n",
    "d_unetr, p_unetr = train_benchmark_model(\"UNETR\", model_unetr, train_loader, val_loader, DEVICE)\n",
    "results[\"UNETR\"] = {\"Path\": p_unetr, \"Paper\": \"Hatamizadeh et al. (2022)\"}\n",
    "del model_unetr; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin UNETR Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs for 3D U-Net\n",
    "\n",
    "# Dataset Limits (Set N_SAMPLES to None for full dataset)\n",
    "N_SAMPLES_SWIN = None     \n",
    "VAL_SPLIT_SWIN = 0.2\n",
    "\n",
    "# Training Hyperparameters for BraTSMamba\n",
    "IMG_SIZE_SWIN = (128, 128, 128)\n",
    "BATCH_SIZE_TRAIN_SWIN = 4   # Effective batch size = 8 (on 2 GPUs)\n",
    "BATCH_SIZE_VAL_SWIN = 1     # MUST be 1 to handle variable image sizes\n",
    "NUM_WORKERS_SWIN = 8        # High worker count for efficient lazy loading\n",
    "NUM_EPOCHS_SWIN = 50\n",
    "LEARNING_RATE_SWIN = 3e-4\n",
    "MAX_RUNTIME_SWIN = 12 * 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Initializing Loaders (Lazy Loading)...\")\n",
    "# Split Data\n",
    "val_count = int(len(all_files) * VAL_SPLIT_SWIN)\n",
    "val_count = max(1, val_count) # Ensure at least 1 val sample\n",
    "train_files, val_files = all_files[val_count:], all_files[:val_count]\n",
    "\n",
    "print(f\"üìä Dataset Split: Train={len(train_files)} | Val={len(val_files)}\")\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE_TRAIN_SWIN, shuffle=True, \n",
    "    num_workers=NUM_WORKERS_SWIN, pin_memory=True\n",
    ")\n",
    "# Note: Batch Size 1 is critical for Val to handle variable volume sizes\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE_VAL_SWIN, shuffle=False, \n",
    "    num_workers=NUM_WORKERS_SWIN, pin_memory=True\n",
    ")\n",
    "print(\"‚úÖ Loaders Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Swin UNETR (SOTA Transformer)\n",
    "\n",
    "model_swin = SwinUNETR(\n",
    "    in_channels=4, \n",
    "    out_channels=4, \n",
    "    feature_size=24, \n",
    "    use_checkpoint=True\n",
    ").to(DEVICE)\n",
    "\n",
    "d_swin, p_swin = train_benchmark_model(\"SwinUNETR\", model_swin, train_loader, val_loader, DEVICE)\n",
    "results[\"SwinUNETR\"] = {\"Path\": p_swin, \"Paper\": \"Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Benchmark model evaluation\n",
    "\n",
    "---\n",
    "\n",
    "Content :\n",
    "\n",
    "- Benchmark Model Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üèÜ FINAL EVALUATION & JSON EXPORT\n",
    "# =============================================================================\n",
    "def get_brats_regions(tensor_oh):\n",
    "    \"\"\"Converts One-Hot -> BraTS Regions (WT, TC, ET)\"\"\"\n",
    "    wt = torch.sum(tensor_oh[:, 1:4, ...], dim=1, keepdim=True) > 0\n",
    "    tc = (tensor_oh[:, 1:2, ...] + tensor_oh[:, 3:4, ...]) > 0\n",
    "    et = tensor_oh[:, 3:4, ...] > 0\n",
    "    return torch.cat([wt, tc, et], dim=1).float()\n",
    "\n",
    "def final_evaluation(results_dict, loader, device):\n",
    "    final_json = []\n",
    "    \n",
    "    metric_dice = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "    metric_hd95 = HausdorffDistanceMetric(include_background=True, percentile=95, reduction=\"mean_batch\")\n",
    "    post_pred = AsDiscrete(argmax=True, to_onehot=4)\n",
    "    post_label = AsDiscrete(to_onehot=4)\n",
    "\n",
    "    for model_name, info in results_dict.items():\n",
    "        print(f\"üìä Evaluating Best {model_name}...\")\n",
    "        \n",
    "        # FIXED: Robust String Matching for Model Re-initialization\n",
    "        if model_name == \"3D_UNet\":\n",
    "            model = UNet(spatial_dims=3, in_channels=4, out_channels=4, channels=(32, 64, 128, 256, 512), strides=(2, 2, 2, 2), num_res_units=2, norm=\"instance\").to(device)\n",
    "        elif model_name == \"UNETR\":\n",
    "            model = UNETR(in_channels=4, out_channels=4, img_size=Config.IMG_SIZE, feature_size=16, hidden_size=768, mlp_dim=3072, num_heads=12, pos_embed=\"perceptron\", norm_name=\"instance\", res_block=True).to(device)\n",
    "        elif model_name == \"SwinUNETR\":\n",
    "            model = SwinUNETR(img_size=Config.IMG_SIZE, in_channels=4, out_channels=4, feature_size=24, use_checkpoint=True).to(device)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unknown model name {model_name}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        model.load_state_dict(torch.load(info[\"Path\"]))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=f\"Eval {model_name}\"):\n",
    "                imgs, lbls = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                \n",
    "                # Sliding Window Inference\n",
    "                logits = sliding_window_inference(imgs, IMG_SIZE, 4, model)\n",
    "                \n",
    "                preds_reg = get_brats_regions(torch.stack([post_pred(i) for i in logits]))\n",
    "                lbls_reg = get_brats_regions(torch.stack([post_label(i) for i in lbls]))\n",
    "                \n",
    "                metric_dice(y_pred=preds_reg, y=lbls_reg)\n",
    "                metric_hd95(y_pred=preds_reg, y=lbls_reg)\n",
    "        \n",
    "        # Aggregation\n",
    "        dice_scores = metric_dice.aggregate().cpu().numpy()\n",
    "        hd95_scores = metric_hd95.aggregate().cpu().numpy()\n",
    "        metric_dice.reset()\n",
    "        metric_hd95.reset()\n",
    "        \n",
    "        entry = {\n",
    "            \"Model\": model_name,\n",
    "            \"Citation\": info[\"Paper\"],\n",
    "            \"Whole_Tumor_Dice\": float(dice_scores[0]),\n",
    "            \"Tumor_Core_Dice\": float(dice_scores[1]),\n",
    "            \"Enhancing_Tumor_Dice\": float(dice_scores[2]),\n",
    "            \"Whole_Tumor_HD95\": float(hd95_scores[0]),\n",
    "            \"Tumor_Core_HD95\": float(hd95_scores[1]),\n",
    "            \"Enhancing_Tumor_HD95\": float(hd95_scores[2]),\n",
    "            \"Mean_Dice\": float(dice_scores.mean())\n",
    "        }\n",
    "        final_json.append(entry)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return final_json\n",
    "\n",
    "# Execute Eval\n",
    "benchmark_data = final_evaluation(results, val_loader, DEVICE)\n",
    "\n",
    "# Save Results\n",
    "json_path = os.path.join(ARTIFACTS_DIR, \"final_benchmark_results.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(benchmark_data, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ Benchmarking Complete. Results saved to: {json_path}\")\n",
    "print(json.dumps(benchmark_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Conclusion & Real-World Deployment Strategy\n",
    "\n",
    "--- \n",
    "\n",
    "Content:\n",
    "\n",
    "- Deployment Scenario\n",
    "- Limitations  \n",
    "- Future Work: BratMamba v2 (Proposed Architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate **BraTSMamba** into a clinical workflow, we propose a \"Human-in-the-Loop\" AI assistance system:\n",
    "1.  **Input:** MRI Technician uploads raw NIfTI sequences (T1, T2, FLAIR) to a hospital PACS server.\n",
    "2.  **Inference:** The model runs on a dedicated on-premise GPU server (ensuring patient data privacy/HIPAA compliance).\n",
    "3.  **Output:** The model generates a candidate segmentation mask.\n",
    "4.  **Review:** A Radiologist reviews the 3D mask, makes minor adjustments if necessary, and approves it for the surgical planning system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Domain Shift:** The model is trained on BraTS (pre-operative) data. It may struggle with post-operative scans containing surgical cavities or metal artifacts unless fine-tuned.\n",
    "  \n",
    "*   **Computational Cost:** While Mamba is efficient ($O(N)$), processing 3D volumes still requires significant GPU memory, limiting deployment on standard clinical desktops without quantization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work: BratMamba v2 (Proposed Architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have theoretically designed and prototyped a more advanced iteration, **BratMamba v2**, which addresses the unidirectional bias of standard Mamba blocks. While the current implementation flattens 3D volumes into 1D sequences, potentially losing spatial coherence, v2 introduces **3D Selective Scan (SS3D)**.\n",
    "\n",
    "**Key Architectural Innovations:**\n",
    "\n",
    "1.  **SS3D (3D Selective Scan):** Inspired by VMamba's 2D scanning, this module scans the 3D volume in **four distinct directions** (Forward, Backward, Transposed-Forward, Transposed-Backward). This ensures that every voxel receives contextual information from all spatial neighbors, not just those preceding it in a flattened sequence.\n",
    "   \n",
    "2.  **GSC (Gated Spatial Convolution):** A specialized block that injects inductive biases (local spatial awareness) before the Mamba layer, ensuring the model captures fine-grained texture details like tumor boundaries better than pure SSMs.\n",
    "   \n",
    "3.  **FUE (Feature Uncertainty Estimation):** A novel module integrated into the skip connections. It calculates the entropy of feature maps to estimate uncertainty, effectively filtering out \"noisy\" or ambiguous features before they merge with the decoder.\n",
    "\n",
    "**Why it's Superior:**\n",
    "\n",
    "Unlike **SegMamba** or **UMamba**, which often rely on standard Mamba blocks, or **Transformers** which suffer from quadratic complexity, BratMamba v2 offers a truly omnidirectional global receptive field with linear complexity. The FUE module specifically targets the \"false positive\" problem common in Edema segmentation.\n",
    "\n",
    "**Current Barrier:**\n",
    "\n",
    "The 4-directional scanning mechanism quadruples the intermediate state memory. Training this architecture requires approximately **60GB VRAM** (e.g., NVIDIA A100 80GB), exceeding the resources available for this hackathon. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 289345111,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
